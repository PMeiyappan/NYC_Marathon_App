{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMPORT LIBRARIES REQUIRED FOR THE CODE\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import sqlite3\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# READ THE CRAWLED CSV FILES AND COMBINE THEM INTO ONE ARRAY\n",
    "\n",
    "def get_field_names(paths):\n",
    "    \"\"\"\n",
    "    Given a list of paths to csv files,\n",
    "    this function will find all the column header names\n",
    "    and return them in the order encountered.\n",
    "    \"\"\"\n",
    "    field_set = set()\n",
    "    fields = []\n",
    "    \n",
    "    for path in paths:\n",
    "        with open(path, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile, delimiter=',')\n",
    "            \n",
    "            for field in reader.fieldnames:\n",
    "                #for every field that we just encountered,\n",
    "                \n",
    "                if field not in field_set:\n",
    "                    #if we have not yet seen this field,\n",
    "                    \n",
    "                    #add the field to the (in order) list\n",
    "                    fields.append(field)\n",
    "                    \n",
    "                    #mark the field as \"seen\"\n",
    "                    field_set.add(field)\n",
    "    return fields\n",
    "\n",
    "\n",
    "def get_relevant_csv_filenames(base_path):\n",
    "    \"\"\"\n",
    "    Given a base path, this function will go through\n",
    "    every file in that directory, and it will return the\n",
    "    files that end with the .csv extension, as a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    paths = []\n",
    "    for filename in os.listdir(base_path):\n",
    "        _,extension = os.path.splitext(filename)\n",
    "\n",
    "        if extension != '.csv':\n",
    "            continue\n",
    "        paths.append(filename)\n",
    "    \n",
    "    return paths\n",
    "    \n",
    "    \n",
    "paths = get_relevant_csv_filenames(base_path='../')\n",
    "paths=['../'+s for s in paths]\n",
    "\n",
    "field_names = get_field_names(paths)\n",
    "print ('field_names:',field_names)\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "for filename in paths:\n",
    "    base,_ = os.path.splitext(filename)\n",
    "    \n",
    "    _,_,year = base.partition('_')\n",
    "    \n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter=',')\n",
    "        \n",
    "        for row in reader:\n",
    "            row['year'] = year\n",
    "            data.append(row)\n",
    "            \n",
    "            #print (row)\n",
    "            #raise Exception()\n",
    "\n",
    "#print (len(data))\n",
    "\n",
    "#Convert the aggregate data into pandas dataframe\n",
    "df=pd.DataFrame(data)\n",
    "print(list(df.columns))\n",
    "\n",
    "# Split age_sex column into two columns of age and sex\n",
    "df['sex']=list(list(zip(*[re.findall(\"[a-zA-Z]+\",i) for i in list(df.sex_age)]))[0])\n",
    "ages=list(list(zip(*[re.findall(\"[0-9]+\",i) for i in list(df.sex_age)]))[0])\n",
    "ages = [int(age) for age in ages]\n",
    "df['age'] = ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PRE-PROCESS DATA: BASIC CLEAN-UP + HANDLE MISSING DATA\n",
    "\n",
    "# Drop columns that are not necessary and inconsistent among time series data\n",
    "df.drop(df.columns[[9,11,13,17,18,22,24]], axis=1,inplace=True) # Note: zero indexed\n",
    "\n",
    "print(list(df.columns))\n",
    "\n",
    "# Remove rows that have missing values\n",
    "df['state'].replace('','Missing',inplace=True)\n",
    "\n",
    "# Find countries that are not United States, and replace the values of corresponding state\n",
    "# Missing\n",
    "cntry_id=[cntry_name!='United States' for cntry_name in df.country]\n",
    "cntry_id=[i for i, x in enumerate(cntry_id) if x] # get indictes\n",
    "\n",
    "df.loc[cntry_id,'state']='Missing'\n",
    "\n",
    "df[:].replace('', np.nan, inplace=True) # Replacing empty strings with NaN\n",
    "\n",
    "#append _ before all column names to avoid problems\n",
    "df.columns='Var_'+df.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STORE THE PROCESSED DATA IN SQL DATABASE\n",
    "\n",
    "\n",
    "# Retain all data including missing elements as those rows are needed later\n",
    "# to find people who ran the marathon multiple times (experience)\n",
    "conn = sqlite3.connect('marathon_runner_data.sqlite')\n",
    "conn.text_factory=str\n",
    "\n",
    "c = conn.cursor()\n",
    "\n",
    "sql_columns = '\\n  , '.join( [('\"%s\" text' % (field,)) for field in df.columns] )\n",
    "\n",
    "table_sql = \"\"\"\n",
    "\n",
    "CREATE TABLE runner_data\n",
    "(\n",
    "    {columns}\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "table_sql = table_sql.format(columns=sql_columns)\n",
    "\n",
    "print (table_sql);\n",
    "c.execute(\"DROP TABLE IF EXISTS runner_data;\")\n",
    "c.execute(table_sql)\n",
    "\n",
    "  \n",
    "for (index,row_series) in df.iterrows():\n",
    "    \n",
    "    sql = \"\"\"\n",
    "    INSERT INTO runner_data ({columns_list})\n",
    "    VALUES ({values})\n",
    "    \"\"\"\n",
    "    columns_list = ', '.join( [('\"%s\"' % (field,)) for field in df.columns] )\n",
    "\n",
    "    #the values should look like this (?,?,?,?,?, ...)\n",
    "    values_question_marks = ['?']*len(row_series)\n",
    "    values_question_marks = ', '.join(values_question_marks)\n",
    "    \n",
    "    sql = sql.format(columns_list=columns_list, \n",
    "                     values=values_question_marks)\n",
    "        \n",
    "    #print(sql) \n",
    "    c.execute(sql, tuple(row_series))\n",
    "#    raise Exception()\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# READ SQL DATABASE AND RUN QUERIES TO CALCULATE \"PRIOR EXPERIENCE\"\n",
    "\n",
    "conn = sqlite3.connect('marathon_runner_data.sqlite')\n",
    "conn.text_factory=str\n",
    "\n",
    "c = conn.cursor()\n",
    "\n",
    "# Add a column named \"experience\" that says how many times each person has run the NYC marathon \n",
    "# and pull out records of people who have run more than once\n",
    "c.execute(\"DROP TABLE IF EXISTS temporary_table\")\n",
    "\n",
    "sql = '''\n",
    "CREATE TABLE temporary_table AS\n",
    "SELECT runner_data.*,experience\n",
    "FROM runner_data\n",
    "JOIN (SELECT Var_first_name,Var_last_name,Var_sex,Var_state,Var_country,\n",
    "Var_age,Var_year,count(*) as experience\n",
    "FROM runner_data\n",
    "GROUP BY Var_first_name,Var_last_name,Var_sex,Var_state,Var_country\n",
    "HAVING count(*)>1) as TEMP\n",
    "\n",
    "ON runner_data.Var_first_name=TEMP.Var_first_name\n",
    "AND runner_data.Var_last_name=TEMP.Var_last_name\n",
    "AND runner_data.Var_sex=TEMP.Var_sex\n",
    "AND runner_data.Var_country=TEMP.Var_country\n",
    "AND runner_data.Var_state=TEMP.Var_state\n",
    "AND runner_data.Var_age<=TEMP.Var_age\n",
    "AND runner_data.Var_year<TEMP.Var_year\n",
    "AND ((TEMP.Var_year-runner_data.Var_year)-(TEMP.Var_age-runner_data.Var_age))<2\n",
    "\n",
    "'''\n",
    "\n",
    "c.execute(sql)\n",
    "#len(c.fetchall())\n",
    "\n",
    "c.execute(\"DROP TABLE IF EXISTS runner_data\")\n",
    "\n",
    "sql='''\n",
    "ALTER TABLE temporary_table \n",
    "RENAME TO runner_data\n",
    "'''\n",
    "c.execute(sql)\n",
    "\n",
    "# Clear memory\n",
    "c.execute(\"DROP TABLE IF EXISTS temporary_table\")\n",
    "\n",
    "# Eliminate data prior to 2006\n",
    "\n",
    "sql= '''\n",
    "DELETE FROM runner_data\n",
    "WHERE Var_year<2006\n",
    "'''\n",
    "c.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GET THE SQL DATABASE INTO DATAFRAME\n",
    "\n",
    "c.execute(\"SELECT * FROM runner_data\")\n",
    "df = pd.DataFrame(c.fetchall())\n",
    "df.columns =[description[0] for description in c.description]\n",
    "\n",
    "# Delete columns you do not require\n",
    "df.drop(['Var_country','Var_first_name','Var_last_name','Var_state',\n",
    "        'Var_age_graded_time','Var_place','Var_place_age',\n",
    "         'Var_place_gender'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DELETE ROWS WITH NAN VALUES\n",
    "df= df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ELIMINATE ROWS WHERE TIME HAS FORMATTING ISSUES\n",
    "\n",
    "col_names=['Var_5km','Var_10km','Var_15km','Var_20km','Var_25km','Var_30km',\n",
    "          'Var_35km','Var_40km','Var_13.1mi','Var_gun_time']\n",
    "\n",
    "for variable in col_names:\n",
    "    typo_time=[len(i) for i in df[variable]]\n",
    "    typo_id=[i for i, x in enumerate(typo_time) if x is not 7] # get indictes\n",
    "    df=df.drop(df.index[typo_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CONVERT ALL TIME VARIABLES FORMATES TO TOTAL MINUTES\n",
    "\n",
    "col_names=['Var_5km','Var_10km','Var_15km','Var_20km','Var_25km','Var_30km',\n",
    "          'Var_35km','Var_40km','Var_13.1mi','Var_gun_time']\n",
    "\n",
    "for col_name in col_names:\n",
    "    #get a list of all the values in column col_name\n",
    "    column_values0 = df.loc[0:][col_name]\n",
    "    \n",
    "    #this is going to hold a new list of transformed\n",
    "    # values\n",
    "    column_values1 = []\n",
    "    \n",
    "    #for each value in the column\n",
    "    for value0 in column_values0:\n",
    "        #split it into hours,minutes,seconds\n",
    "        #print(value0)\n",
    "\n",
    "        HH,MM,SS = value0.split(':')\n",
    "        \n",
    "        #compute the float total of minutes\n",
    "        value1 = float(HH)*60 + float(MM) + float(SS)/60\n",
    "        \n",
    "        #append it to the new list of column-values\n",
    "        column_values1 += [value1]\n",
    "    #the loop has finished, we now have the new list of column values\n",
    "    \n",
    "    #therefore assign back to the table\n",
    "    df[col_name] = column_values1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert ALL DATA TYPES TO FLOAT\n",
    "\n",
    "datatype_convert=['Var_year','Var_age']\n",
    "df[datatype_convert] = df[datatype_convert].astype(float)\n",
    "datatype_convert=[];\n",
    "\n",
    "# Create Y vector\n",
    "col_names=['Var_5km','Var_10km','Var_15km','Var_20km','Var_25km','Var_30km',\n",
    "          'Var_35km','Var_40km','Var_13.1mi']\n",
    "\n",
    "Y=df[col_names]\n",
    "\n",
    "# Drop col_names from df\n",
    "df.drop(col_names, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CONVERT CUMULATIVE SPLIT TIMES INTO TIME IN EACH SPLIT\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "Y.loc[:,'Var_Finish']=df['Var_gun_time']-Y['Var_40km']\n",
    "Y.loc[:,'Var_40km']=Y['Var_40km']-Y['Var_35km']\n",
    "Y.loc[:,'Var_35km']=Y['Var_35km']-Y['Var_30km']\n",
    "Y.loc[:,'Var_30km']=Y['Var_30km']-Y['Var_25km']\n",
    "Y.loc[:,'Var_25km']=Y['Var_25km']-Y['Var_13.1mi']\n",
    "Y.loc[:,'Var_13.1mi']=Y['Var_13.1mi']-Y['Var_20km']\n",
    "Y.loc[:,'Var_20km']=Y['Var_20km']-Y['Var_15km']\n",
    "Y.loc[:,'Var_15km']=Y['Var_15km']-Y['Var_10km']\n",
    "Y.loc[:,'Var_10km']=Y['Var_10km']-Y['Var_5km']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CONVERT RESPONSE DATA TO FRACTIONS\n",
    "\n",
    "Y=Y.divide(df['Var_gun_time'],axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# READ AND STORE THE WEATHER DATA IN SQL DATABASE\n",
    "\n",
    "\n",
    "# The raw data is in csv format and retrieved from NLDAS data\n",
    "# from NASA Giovanni interactive database\n",
    "\n",
    "# Read csv file\n",
    "data_directory='../Weather_data_raw/'\n",
    "paths = get_relevant_csv_filenames(base_path=data_directory)\n",
    "paths=[data_directory+path for path in paths]\n",
    "field_names = get_field_names(paths)\n",
    "print ('field_names:',field_names)\n",
    "\n",
    "data = []\n",
    "\n",
    "for filename in paths:\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter=',')\n",
    "        \n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "            \n",
    "            #print (row)\n",
    "            #raise Exception()\n",
    "\n",
    "print (len(data))\n",
    "\n",
    "#Convert the aggregate data into pandas dataframe\n",
    "wdf=pd.DataFrame(data)\n",
    "print(list(wdf.columns))\n",
    "\n",
    "conn = sqlite3.connect('weather_data.sqlite')\n",
    "conn.text_factory=str\n",
    "\n",
    "c = conn.cursor()\n",
    "\n",
    "sql_columns = '\\n  , '.join( [('\"%s\" text' % (field,)) for field in wdf.columns] )\n",
    "\n",
    "table_sql = \"\"\"\n",
    "\n",
    "CREATE TABLE marathon_weather_data\n",
    "(\n",
    "    {columns}\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "table_sql = table_sql.format(columns=sql_columns)\n",
    "\n",
    "print (table_sql);\n",
    "c.execute(\"DROP TABLE IF EXISTS marathon_weather_data;\")\n",
    "c.execute(table_sql)\n",
    "\n",
    "  \n",
    "for (index,row_series) in wdf.iterrows():\n",
    "    \n",
    "    sql = \"\"\"\n",
    "    INSERT INTO marathon_weather_data ({columns_list})\n",
    "    VALUES ({values})\n",
    "    \"\"\"\n",
    "    columns_list = ', '.join( [('\"%s\"' % (field,)) for field in wdf.columns] )\n",
    "\n",
    "    #the values should look like this (?,?,?,?,?, ...)\n",
    "    values_question_marks = ['?']*len(row_series)\n",
    "    values_question_marks = ', '.join(values_question_marks)\n",
    "    \n",
    "    sql = sql.format(columns_list=columns_list, \n",
    "                     values=values_question_marks)\n",
    "        \n",
    "    #print(sql) \n",
    "    c.execute(sql, tuple(row_series))\n",
    "#    raise Exception()\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# READ WEATHER DATA AND APPEND AS FEATURES \n",
    "\n",
    "conn = sqlite3.connect('weather_data.sqlite')\n",
    "conn.text_factory=str\n",
    "\n",
    "d = conn.cursor()\n",
    "\n",
    "d.execute(\"SELECT * FROM marathon_weather_data\")\n",
    "wdf = pd.DataFrame(d.fetchall())\n",
    "wdf.columns =[description[0] for description in d.description]\n",
    "\n",
    "# Change data types\n",
    "wdf[wdf.columns] = wdf[wdf.columns].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate weather conditions for each person based on their timings and year\n",
    "\n",
    "gun_time_hours=list((df['Var_gun_time']/60))\n",
    "\n",
    "for i,data in enumerate(gun_time_hours):\n",
    "    # Get the marathon day's data in a new temporary dataframe \n",
    "    # Waves start at 9:45, but adding one hour as they arrive early\n",
    "    wdf_temp=wdf.loc[wdf['Year'].isin([df.loc[df.index[i],'Var_year']])]\n",
    "    wdf_temp.loc[wdf_temp['hour'].isin(range(9,11+int(data)))\n",
    "                                            ,'Rel. Humidity'].mean(axis=0)\n",
    "    df.loc[df.index[i],'Rel. Humidity']= wdf_temp.loc[wdf_temp['hour'].isin(range(9,11+int(data)))\n",
    "                                            ,'Rel. Humidity'].mean(axis=0)\n",
    "    df.loc[df.index[i],'Temperature']= wdf_temp.loc[wdf_temp['hour'].isin(range(9,11+int(data)))\n",
    "                                            ,'tmp2m'].mean(axis=0)\n",
    "    df.loc[df.index[i],'X_wind']= wdf_temp.loc[wdf_temp['hour'].isin(range(9,11+int(data)))\n",
    "                                            ,'ugrdh'].mean(axis=0)\n",
    "    df.loc[df.index[i],'Y_wind']= wdf_temp.loc[wdf_temp['hour'].isin(range(9,11+int(data)))\n",
    "                                            ,'vgrd1'].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop unecessary column: Delete year column you no more require\n",
    "\n",
    "df.drop(['Var_year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dummy code categorical variables \n",
    "\n",
    "dummy_code_var=['Var_sex']\n",
    "dummy_data= pd.DataFrame()\n",
    "\n",
    "for dummy_var in dummy_code_var:\n",
    "    dummies=pd.get_dummies(df[dummy_var])\n",
    "    dummy_data = pd.concat([dummy_data, dummies], axis=1)\n",
    "    # Drop the column from df dataframe\n",
    "    df.drop([dummy_var], inplace=True, axis=1)\n",
    "    # Avoiding dummy-variable trap\n",
    "    dummy_data.drop([list(dummies.columns.values)[1]], inplace=True, axis=1)\n",
    "\n",
    "dummy_data=dummy_data.as_matrix()\n",
    "df=df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This information is not used in the model. But, is useful for later interpretation.\n",
    "# These variables relate to course information by splits\n",
    "\n",
    "'''\n",
    "# Course conditions (NYC) for each split time\n",
    "Distance=[5,5,5,5,1.08,3.92,5,5,5,2.2]\n",
    "\n",
    "Elevation_gain=[178.0382269,69.11468055,96.66463271,62.74108641,41.21141586,\n",
    "                131.4207563,105.3092448,64.24585234,130.97302,58.67416456]\n",
    "\n",
    "Elevation_loss=[-197.0382269,-116.7028596,-97.68808331,-106.0469012,\n",
    "                -42.6731828,-38.04677694,-154.8496651, -72.23193359,\n",
    "                -49.4847453,-46.70543204]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This part of the code is to runa t-SNE algorithm for \n",
    "# dimensionality reduction of split time data and \n",
    "# visualize the data in relation to experience\n",
    "\n",
    "import random\n",
    "import tsne\n",
    "import pylab as Plot\n",
    "\n",
    "\n",
    "exp=list(df[:,2])\n",
    "\n",
    "# Pick random sample with experience range\n",
    "rand_item = random.sample([i for i,y in enumerate(exp) if y<=2],2000)\n",
    "rand_item = rand_item+random.sample([i for i,y in enumerate(exp) if y<=8 and y>5],2000)\n",
    "#rand_item = rand_item+random.sample([i for i,y in enumerate(exp) if y<=5 and y>3],1000)\n",
    "#rand_item = rand_item+random.sample([i for i,y in enumerate(exp) if y<=10 and y>5],1000)\n",
    "rand_item = rand_item+random.sample([i for i,y in enumerate(exp) if y>16],2000)\n",
    "\n",
    "y=Y.as_matrix()[rand_item,:]\n",
    "y = tsne.tsne(y, 2, 20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PLOT DATA FROM t-SNE ANALYSIS\n",
    "\n",
    "labels=['Newbie','Intermediate','Experienced']#,'3-5 years','5-10 years','>10 years']\n",
    "colors = ['b', 'g', 'r'] #y', 'm', 'r']\n",
    "\n",
    "a = plt.scatter(y[0:1999,0], y[0:1999,1], 3, color=colors[0])\n",
    "b = plt.scatter(y[2000:3999,0], y[2000:3999,1], 3, color=colors[1])\n",
    "c  = plt.scatter(y[4000:5999,0], y[4000:5999,1], 3, color=colors[2])\n",
    "#d  = plt.scatter(y[3000:3999,0], y[3000:3999,1], 10, color=colors[3])\n",
    "#e  = plt.scatter(y[4000:4999,0], y[4000:4999,1], 10, color=colors[4])\n",
    "\n",
    "plt.legend((a,b,c),tuple(labels),scatterpoints=1,\n",
    "           loc='lower left',ncol=3, fontsize=10)\n",
    "plt.title('t-SNE analysis on NYC Marathon Split Times')\n",
    "\n",
    "plt.savefig('t-SNE_Marathon.png',dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save scaling information for later use\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(df)\n",
    "joblib.dump(scaler, 'scaling_variables.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OUTPUT DATA FOR CHECK-POINT AND EASY RETRIEVAL\n",
    "\n",
    "# Standardize all continuous variables\n",
    "X = preprocessing.scale(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create polynomial features of order 2: Non-linear and interaction effects\n",
    "poly = PolynomialFeatures(2)\n",
    "X=poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge categorical and continuous data\n",
    "X=np.concatenate([X,dummy_data],axis=1)\n",
    "\n",
    "# Response fractional matrix\n",
    "y=Y.as_matrix()\n",
    "\n",
    "#Dump the data to csv for easy retrieval..The data is small\n",
    "np.savetxt(\"Features_Standardized.csv\", X, delimiter=\",\")\n",
    "np.savetxt(\"Response_fractions.csv\", y, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare X and y data and local weights for model fitting\n",
    "# taken out ele loss, prediction si 100%\n",
    "# when ele gain is taken out the predicrtion is 100%\n",
    "\n",
    "course_info=np.transpose(np.repeat([Distance,Elevation_gain,Elevation_loss\n",
    "                                    ],Y.shape[0],axis=1))\n",
    "\n",
    "X=np.concatenate((np.tile(df,(Y.shape[1],1)),course_info),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize all continuous variables\n",
    "X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create polynomial features of order 2: Non-linear and interaction effects\n",
    "poly = PolynomialFeatures(2)\n",
    "X=poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge categorical and continuous data\n",
    "X=np.concatenate([X,np.tile(dummy_data,(Y.shape[1],1))],axis=1)\n",
    "\n",
    "# Flatten y for numpy to interpret correctly\n",
    "y=np.ravel(np.repeat(range(Y.shape[1]),Y.shape[0]))\n",
    "\n",
    "weights=np.repeat(np.transpose(Y.as_matrix()),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Run the multinomial Logistic regression from scikit-learn\n",
    "# This was just an experiemntation case\n",
    "\n",
    "model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, \n",
    "                           fit_intercept=True, class_weight=None, \n",
    "                           random_state=None, solver='lbfgs', max_iter=100000, \n",
    "                           multi_class='multinomial', verbose=0, warm_start=False)\n",
    "\n",
    "model = model.fit(X, y,sample_weight=weights)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "model.score(X, y,sample_weight=weights)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# evaluate the model using 10-fold cross-validation\n",
    "scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=3)\n",
    "print scores\n",
    "print scores.mean()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MACHINE LEARNING ALGORITHM\n",
    "\n",
    "\n",
    "#Run the Fractional Multinomial Logistic Regression model\n",
    "#This model is a multinomial generalization of the fractional\n",
    "#logit model proposed by Papke and Woolridge in 1996\n",
    "\n",
    "# Requires Python wrapper to Glmnet package available at \n",
    "#https://github.com/dwf/glmnet-python\n",
    "    \n",
    "    \n",
    "from glmnet import LogisticNet\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "display_bar = '-'*70\n",
    " \n",
    "X=df\n",
    "y=np.ones((len(X),), dtype=np.int)\n",
    "y[len(y)-1]=0\n",
    "y[len(y)-2]=2\n",
    "\n",
    "#y=np.repeat(np.transpose(Y.as_matrix()),1)\n",
    "\n",
    "print display_bar\n",
    "print \"Fit a logistic net on marathon data\"\n",
    "print display_bar\n",
    "\n",
    "lognet = LogisticNet(alpha=0.5)\n",
    "lognet.fit(X, y)\n",
    "\n",
    "print lognet\n",
    " \n",
    "print display_bar\n",
    "print \"Predictions for the last logistic net model:\"\n",
    "print display_bar\n",
    "\n",
    "preds = lognet.predict(X)\n",
    "print preds[:10,np.shape(preds)[1]-1]\n",
    "\n",
    "lognet.plot_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PLOT FREQUENCY DISTRIBUTION OF AVERAGE RUNNING TIME BY AGE AND GENDER\n",
    "\n",
    "N = len(df.groupby(\"Year1\"))\n",
    "menMeans = list(df.groupby([\"year1\",\"sex\"]).mean().gun_time)[1::2]\n",
    "menStd = list(df.groupby([\"year1\",\"sex\"]).mean().gun_time)[1::2]\n",
    "\n",
    "womenMeans = list(df.groupby([\"year1\",\"sex\"]).mean().gun_time)[::2]\n",
    "womenStd = list(df.groupby([\"year1\",\"sex\"]).mean().gun_time)[::2]\n",
    "\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, menMeans, width, color='r', yerr=menStd)\n",
    "\n",
    "\n",
    "rects2 = ax.bar(ind + width, womenMeans, width, color='y', yerr=womenStd)\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Time in minutes')\n",
    "ax.set_title('Marathon finish time by gender')\n",
    "ax.set_xticks(ind + width)\n",
    "ax.set_xticklabels(tuple(np.unique(df[\"year1\"])))\n",
    "\n",
    "ax.legend((rects1[0], rects2[0]), ('Men', 'Women'))\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    # attach some text labels\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                '%d' % int(height),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
